{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic scoring re-definition in terms of DL\n",
    "**There are a number of attributes of the essay we want to analyze. They are**\n",
    "- Grammar - No / Low grammatical mistakes\n",
    "- Sentence structures - Coherent sentence structures\n",
    "- Coherent & logical reasoning of the concept\n",
    "- Consistency of the focus on topic in essay\n",
    "- Decent length of the essay\n",
    "- Supporting arguments & numbers/stats to emphasis your view point\n",
    "\n",
    "Since DL models can't directly give us this info unless specifically trained for the task. We need to go ahead with some assumptions to make it work. \n",
    "- Small scale LLMs are not able to judge the essay w.r.t these parameters. Always giving more info than necessary & many times wrong info being shared. So ruling out LLMs to generate data at this point.\n",
    "- For grammatical mistakes - Word mistakes can be identified with a good english dictionary, sentence structure mistakes can be inferred by passing a word embedding list through a LSTM and providing that context to the model while grading. Or we can use online sentence correction tools to identify if sentences require a large amount of corrections or not.\n",
    "- Subject verb agreement could be a starter for grammar\n",
    "- Logical reasoning can't be directly inferred, so using sentences embeddings in a sequence can be processed to estimate it in abstract way\n",
    "- Consitency of focus on topic also will have to be done through sentence embeddings in a sequence\n",
    "- Length of the essay as separate feature\n",
    "- Numbers, percentages, metrics etc. if we can identify them and have features related to existance & extant of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giridhar/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gotutiyan/gec-t5-small-clang8\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gotutiyan/gec-t5-small-clang8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.configuration_utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"gotutiyan/gec-t5-small-clang8\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 1024,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 8,\n",
       "  \"num_heads\": 6,\n",
       "  \"num_layers\": 8,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.43.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many people have car where they live. The thing they don't know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban's families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won't see a car in Vauban's streets because they are completely \"car free\" but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called \"smart planning\". The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that \"All of our development since World war 2 has been centered on the cars,and that will have to change\" and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don't need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called \"car reduced\"communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.    \n"
     ]
    }
   ],
   "source": [
    "print(train_data['full_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\"The thing they don't know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban's families do not own cars,and 57 percent sold a car to move there.\",\n",
    "                      return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> The thing they don't know is that when you use a car a\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "model.generate(inputs=tokenized['input_ids'])[0],min_length=100, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    37,   589,    79,   278,     3,    31,     3,    17,   214,\n",
       "            19,    24,   116,    25,   169,     3,     9,   443,     3,     9,\n",
       "           418,    13,   378,    54,  1837,   114,    25,    54,   129,    16,\n",
       "            46,  3125,    42,     8,  7269,    24,     8,   443,    65,    19,\n",
       "          1282,    12, 13418,    30,     3,    99,   841,    19,  3214,     3,\n",
       "             6,    68,    16,   584,  6727, 25534,     3,     6,  3434,    79,\n",
       "           103,    59,    43,    24,   682,   250,  2861,  1093,    13,   409,\n",
       "            76,  3478,     3,    31,     7,  1791,   103,    59,   293,  2948,\n",
       "             3,     6,    11,     3,  3436,  1093,  1789,     3,     9,   443,\n",
       "            12,   888,   132,     3,     5,     1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(inputs=tokenized['input_ids'],max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alice-hml/mBERT_grammatical_error_tagger\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"alice-hml/mBERT_grammatical_error_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\"The thing they don't know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban's families do not own cars,and 57 percent sold a car to move there.\",\n",
    "                      return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.vocab\n",
    "reverse_vocabulary = {value:key for key,value in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10117,\n",
       " 40414,\n",
       " 10689,\n",
       " 16938,\n",
       " 112,\n",
       " 188,\n",
       " 21852,\n",
       " 10124,\n",
       " 10189,\n",
       " 10841,\n",
       " 13028,\n",
       " 11760,\n",
       " 169,\n",
       " 13000,\n",
       " 10164,\n",
       " 11290,\n",
       " 10108,\n",
       " 40414,\n",
       " 10944,\n",
       " 84630,\n",
       " 11850,\n",
       " 13028,\n",
       " 10944,\n",
       " 15329,\n",
       " 10106,\n",
       " 13621,\n",
       " 65074,\n",
       " 10123,\n",
       " 10345,\n",
       " 10105,\n",
       " 100332,\n",
       " 10189,\n",
       " 10105,\n",
       " 13000,\n",
       " 10393,\n",
       " 10124,\n",
       " 15838,\n",
       " 10114,\n",
       " 33989,\n",
       " 54006,\n",
       " 10135,\n",
       " 12277,\n",
       " 30455,\n",
       " 10124,\n",
       " 33734,\n",
       " 10473,\n",
       " 10106,\n",
       " 69342,\n",
       " 82439,\n",
       " 41275,\n",
       " 117,\n",
       " 12775,\n",
       " 10689,\n",
       " 11758,\n",
       " 10529,\n",
       " 10189,\n",
       " 11284,\n",
       " 11203,\n",
       " 12373,\n",
       " 10923,\n",
       " 22362,\n",
       " 10108,\n",
       " 10321,\n",
       " 105807,\n",
       " 112,\n",
       " 187,\n",
       " 15300,\n",
       " 10149,\n",
       " 10472,\n",
       " 12542,\n",
       " 24602,\n",
       " 117,\n",
       " 10111,\n",
       " 11817,\n",
       " 22362,\n",
       " 15337,\n",
       " 169,\n",
       " 13000,\n",
       " 10114,\n",
       " 18577,\n",
       " 11155,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0\n",
      "The 0\n",
      "thing 0\n",
      "they 0\n",
      "don 0\n",
      "' 0\n",
      "t 0\n",
      "know 0\n",
      "is 0\n",
      "that 0\n",
      "when 0\n",
      "you 0\n",
      "use 0\n",
      "a 0\n",
      "car 0\n",
      "al 0\n",
      "##ot 0\n",
      "of 0\n",
      "thing 0\n",
      "can 0\n",
      "happen 0\n",
      "like 0\n",
      "you 0\n",
      "can 0\n",
      "get 0\n",
      "in 0\n",
      "ac 0\n",
      "##cide 0\n",
      "##t 0\n",
      "or 0\n",
      "the 0\n",
      "smoke 0\n",
      "that 0\n",
      "the 0\n",
      "car 0\n",
      "has 0\n",
      "is 0\n",
      "bad 0\n",
      "to 0\n",
      "br 0\n",
      "##eath 0\n",
      "on 0\n",
      "if 0\n",
      "someone 0\n",
      "is 0\n",
      "walk 0\n",
      "but 0\n",
      "in 0\n",
      "VA 0\n",
      "##UB 0\n",
      "##AN 0\n",
      ", 0\n",
      "Germany 0\n",
      "they 0\n",
      "dont 0\n",
      "have 0\n",
      "that 0\n",
      "pro 0\n",
      "##ble 0\n",
      "because 0\n",
      "70 0\n",
      "percent 0\n",
      "of 0\n",
      "va 0\n",
      "##uban 0\n",
      "' 0\n",
      "s 0\n",
      "families 0\n",
      "do 0\n",
      "not 1\n",
      "own 2\n",
      "cars 0\n",
      ", 0\n",
      "and 0\n",
      "57 0\n",
      "percent 0\n",
      "sold 0\n",
      "a 0\n",
      "car 0\n",
      "to 0\n",
      "move 0\n",
      "there 0\n",
      ". 0\n",
      "[SEP] 0\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenized['input_ids'][0].tolist(), out['logits'][0].argmax(dim=1).tolist()):\n",
    "    print(reverse_vocabulary[token],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypothesis - 1 \n",
    "    - More number of misspelt words in essays which got scored highly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dict = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import swifter\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"\n",
    "    Removes any special characters or number from text, simply returns all words in lower case\n",
    "    \"\"\"\n",
    "    return re.sub('\\s+', ' ', re.sub('[^A-Za-z]', ' ', text)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8d215ac78044f78a155fc5e46ace25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['full_text_cleaned'] = train_data['full_text'].swifter.apply(remove_special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17ebfeae5dd48cbb5a0159c4163bdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['misspelt_words'] = train_data['full_text_cleaned'].swifter.apply(lambda x: [word for word in x.split() if not english_dict.check(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['length_of_misspelt'] = train_data['misspelt_words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156.0</td>\n",
       "      <td>26.153846</td>\n",
       "      <td>13.154248</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>970.0</td>\n",
       "      <td>23.869072</td>\n",
       "      <td>12.007410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3926.0</td>\n",
       "      <td>20.658176</td>\n",
       "      <td>11.024157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6280.0</td>\n",
       "      <td>17.363694</td>\n",
       "      <td>9.854664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4723.0</td>\n",
       "      <td>14.533983</td>\n",
       "      <td>9.242295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252.0</td>\n",
       "      <td>19.308307</td>\n",
       "      <td>12.041993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count       mean        std  min   25%   50%   75%    max\n",
       "score                                                            \n",
       "6       156.0  26.153846  13.154248  3.0  16.0  23.0  35.0   70.0\n",
       "5       970.0  23.869072  12.007410  1.0  15.0  23.0  30.0   84.0\n",
       "4      3926.0  20.658176  11.024157  0.0  13.0  19.0  26.0  103.0\n",
       "3      6280.0  17.363694   9.854664  0.0  10.0  16.0  22.0   90.0\n",
       "2      4723.0  14.533983   9.242295  0.0   8.0  13.0  19.0   92.0\n",
       "1      1252.0  19.308307  12.041993  1.0  11.0  17.0  24.0  149.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data.groupby('score')['length_of_misspelt'].describe().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypothesis 1 is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypotheis 2\n",
    "    - More number of Nouns used in higher score essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bc87e0155a409e887befcfb81b46ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/swifter/swifter.py:311\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[0;32m--> 311\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03mand can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mis preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_doc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswifter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/swifter/swifter.py:329\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_apply(func, convert_dtype, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# use pandas\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pandas_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/swifter/swifter.py:235\u001b[0m, in \u001b[0;36mSeriesAccessor._pandas_apply\u001b[0;34m(self, df, func, convert_dtype, *args, **kwds)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar:\n\u001b[1;32m    234\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar_desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas Apply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1120\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/tokenizer.pyx:158\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/tokenizer.pyx:194\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/tokenizer.pyx:398\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/tokenizer.pyx:477\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/vocab.pyx:166\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/vocab.pyx:203\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/lang/lex_attrs.py:144\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m             shape\u001b[38;5;241m.\u001b[39mappend(shape_char)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprefix\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data['spacy_doc'] = train_data['full_text'].swifter.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_pos_tags(text):\n",
    "    \"\"\"\n",
    "    Returns the frequency of each part of speech tag in the text\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos_tags = {}\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_tags:\n",
    "            pos_tags[token.pos_] += 1\n",
    "        else:\n",
    "            pos_tags[token.pos_] = 1\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e13a1e2a834417a4ad5e94a392fb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_pos_tags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36mget_pos_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pos_tags\u001b[39m(text):\n\u001b[0;32m----> 3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m [(token\u001b[38;5;241m.\u001b[39mtext, token\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pos_tags\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/pipeline/attributeruler.py:142\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    140\u001b[0m error_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations(doc, matches)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Documents/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/pipeline/attributeruler.py:149\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: Doc):\n\u001b[0;32m--> 149\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_spans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     matches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m         (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data['pos_tags'] = train_data['full_text'].progress_apply(get_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'live.': 161,\n",
       " 'alot': 2056,\n",
       " 'accidet': 2,\n",
       " 'VAUBAN,Germany': 6,\n",
       " 'dont': 4073,\n",
       " 'proble': 8,\n",
       " '70': 261,\n",
       " \"vauban's\": 18,\n",
       " 'cars,and': 13,\n",
       " '57': 206,\n",
       " 'there.': 1346,\n",
       " 'parkig': 2,\n",
       " ',driveways': 2,\n",
       " 'freiburd': 1,\n",
       " 'Swiss': 55,\n",
       " 'borders.': 35,\n",
       " 'probaly': 117,\n",
       " \"Vauban's\": 258,\n",
       " '\"car': 176,\n",
       " 'free\"': 29,\n",
       " 'VAUBAN': 22,\n",
       " 'allowed,but': 5,\n",
       " 'development,where': 8,\n",
       " '$40,000': 109,\n",
       " 'home.': 412,\n",
       " 'vauban': 24,\n",
       " '2006': 35,\n",
       " ',they': 15,\n",
       " 'Europe,The': 1,\n",
       " 'untile': 3,\n",
       " '\"smart': 129,\n",
       " 'planning\".': 28,\n",
       " 'tailes': 1,\n",
       " 'passengee': 1,\n",
       " '12': 471,\n",
       " 'Europe': 1099,\n",
       " '50': 430,\n",
       " 'States.': 573,\n",
       " 'honeslty': 9,\n",
       " 'Vaudan': 2,\n",
       " '5,500': 104,\n",
       " 'mile.': 18,\n",
       " 'artical': 343,\n",
       " 'David': 94,\n",
       " '\"All': 70,\n",
       " 'change\"': 20,\n",
       " 'beacuse': 265,\n",
       " 'thats': 1543,\n",
       " 'bycles': 1,\n",
       " 'car.': 3378,\n",
       " 'thik': 11,\n",
       " 'to.': 857,\n",
       " ',the': 25,\n",
       " 'reduced\"communtunties,and': 1,\n",
       " 'act,if': 4,\n",
       " 'cautiously.': 21,\n",
       " 'Maany': 1,\n",
       " 'year.': 282,\n",
       " 'bill,80': 1,\n",
       " '20': 161,\n",
       " 'transports.': 7,\n",
       " 'this.': 814,\n",
       " 'NASA': 6581,\n",
       " '\"face\"': 461,\n",
       " 'mars.': 464,\n",
       " 'form.': 219,\n",
       " 'isue': 5,\n",
       " 'that.': 1279,\n",
       " 'off,': 306,\n",
       " 'martions': 4,\n",
       " 'drawing.': 5,\n",
       " 'of,': 115,\n",
       " 'life.': 1561,\n",
       " 'martians.': 9,\n",
       " 'martion': 9,\n",
       " 'big.': 27,\n",
       " 'Next,': 278,\n",
       " 'landform.': 2364,\n",
       " 'landforms': 976,\n",
       " 'America,': 213,\n",
       " 'Earth.': 2195,\n",
       " 'human...': 1,\n",
       " 'to?': 44,\n",
       " 'course!': 4,\n",
       " 'not?': 68,\n",
       " 'landform': 2755,\n",
       " 'face.': 2410,\n",
       " 'now?': 102,\n",
       " 'Finaly,': 9,\n",
       " 'me.': 500,\n",
       " \"i've\": 42,\n",
       " 'say,': 248,\n",
       " 'beleive': 234,\n",
       " 'sculpture.': 6,\n",
       " 'pictures.': 144,\n",
       " 'movies,': 117,\n",
       " 'media.': 40,\n",
       " 'However,': 1291,\n",
       " 'have.': 563,\n",
       " 'decades,': 63,\n",
       " 'everyone.': 303,\n",
       " 'now,': 480,\n",
       " 'convey.': 8,\n",
       " 'Currently,': 36,\n",
       " '\"driverless\".': 10,\n",
       " 'driving,': 647,\n",
       " 'situations.': 206,\n",
       " '\"driverless\"': 174,\n",
       " 'improve,': 17,\n",
       " 'driverless': 15211,\n",
       " 'neccessary.': 17,\n",
       " 'subway.': 8,\n",
       " 'problem.': 403,\n",
       " 'masnufacturers': 1,\n",
       " '\"fun\",': 2,\n",
       " '\"fun\"': 11,\n",
       " 'go.': 598,\n",
       " 'disaster.': 51,\n",
       " 'cars.': 4790,\n",
       " 'semi-automatic': 4,\n",
       " 'popular,': 39,\n",
       " 'laws.': 104,\n",
       " 'accident.': 645,\n",
       " 'new,': 101,\n",
       " 'themselves.': 615,\n",
       " 'them,': 668,\n",
       " 'system.': 1461,\n",
       " 'manufactuers': 22,\n",
       " 'debt,': 17,\n",
       " 'develope': 63,\n",
       " 'diasaster': 2,\n",
       " 'making.': 37,\n",
       " 'transportations': 39,\n",
       " 'yourself,': 68,\n",
       " 'end,': 167,\n",
       " 'time.': 2290,\n",
       " 'benefical.': 4,\n",
       " 'Venus,': 1635,\n",
       " 'earthquakes,': 362,\n",
       " '800': 1424,\n",
       " 'Fahrenheit': 211,\n",
       " 'futur': 6,\n",
       " 'article,': 1219,\n",
       " 'Venus.': 4208,\n",
       " 'Venus': 21637,\n",
       " 'climate.': 23,\n",
       " 'author:': 2,\n",
       " '3)': 110,\n",
       " '\"A': 776,\n",
       " '97': 837,\n",
       " 'Venus’s': 21,\n",
       " 'atmosphere.': 612,\n",
       " 'surface,': 579,\n",
       " 'Fahrenheit....Beyond': 1,\n",
       " 'heat,': 277,\n",
       " 'Venusian': 368,\n",
       " 'volcanoes,': 337,\n",
       " 'surface.\"': 68,\n",
       " 'truth,': 34,\n",
       " 'continuously.': 2,\n",
       " 'distance.': 292,\n",
       " 'humans.': 650,\n",
       " 'author,': 115,\n",
       " '(studying': 2,\n",
       " 'Venus)': 8,\n",
       " '(5)': 14,\n",
       " '\"NASA’s': 3,\n",
       " 'fray.': 193,\n",
       " 'blimp-like': 270,\n",
       " '30': 680,\n",
       " 'landscape.': 149,\n",
       " 'storms,': 109,\n",
       " 'way.': 1107,\n",
       " 'thirty-plus': 121,\n",
       " '170': 488,\n",
       " 'Fahrenheit,': 477,\n",
       " 'Earth.\"': 244,\n",
       " 'evidence,': 90,\n",
       " 'reasonning,': 1,\n",
       " \"NASA's\": 678,\n",
       " 'credibility.': 5,\n",
       " 'also,': 114,\n",
       " 'planet.': 2248,\n",
       " 'conclusion,': 1983,\n",
       " 'concession,': 2,\n",
       " 'things.': 994,\n",
       " 'all,': 1021,\n",
       " 'Humans.': 3,\n",
       " 'Dear,': 88,\n",
       " 'College.\"There': 1,\n",
       " 'College\"': 55,\n",
       " 'anachronism,': 59,\n",
       " 'possible,': 149,\n",
       " 'vote,': 754,\n",
       " '(by': 41,\n",
       " 'population)': 41,\n",
       " 'virue': 2,\n",
       " 'mal': 4,\n",
       " 'Constitution.': 54,\n",
       " 'because,it': 5,\n",
       " 'anachronism.': 34,\n",
       " 'non-democratic': 138,\n",
       " '[overruled]': 20,\n",
       " 'canaditdate': 1,\n",
       " 'populare': 10,\n",
       " 'winner.': 187,\n",
       " 'sense.It': 1,\n",
       " ',not': 8,\n",
       " 'people.': 2138,\n",
       " '(and': 67,\n",
       " 'betrayed).': 5,\n",
       " 'Another,': 8,\n",
       " 'because,': 567,\n",
       " 'possible.': 380,\n",
       " 'vote.': 1522,\n",
       " \"canadate's\": 1,\n",
       " 'least,': 101,\n",
       " 'attintion': 8,\n",
       " 'canadidates': 7,\n",
       " 'does.': 191,\n",
       " 'state.': 565,\n",
       " 'election,': 319,\n",
       " 'that,': 1312,\n",
       " '1/2': 5,\n",
       " 'American': 1221,\n",
       " \"[2012's]\": 16,\n",
       " 'election.': 734,\n",
       " 'From,': 21,\n",
       " 'PROPER_NAME': 273,\n",
       " 'it,': 1578,\n",
       " 'it.': 5800,\n",
       " 'americans.': 6,\n",
       " 'congress.': 106,\n",
       " 'sabaotage': 1,\n",
       " 'commited': 11,\n",
       " 'president.': 1979,\n",
       " 'citizens.': 291,\n",
       " 'passage,': 276,\n",
       " 'corrupt.': 11,\n",
       " 'be:': 5,\n",
       " '1960.': 31,\n",
       " 'segregationalists': 2,\n",
       " 'Louisiana': 120,\n",
       " 'relpacing': 1,\n",
       " 'F.': 189,\n",
       " 'Kennedy.': 130,\n",
       " '(So': 21,\n",
       " 'Kennedy': 112,\n",
       " 'Kennedy.)': 10,\n",
       " '-source': 2,\n",
       " '2,': 333,\n",
       " 'paragraph.': 59,\n",
       " '1960': 73,\n",
       " 'electors.': 926,\n",
       " 'candidate.': 314,\n",
       " 'conected': 3,\n",
       " 'everyones': 146,\n",
       " 'states.': 777,\n",
       " 'say.': 178,\n",
       " 'voters,': 218,\n",
       " 'american': 166,\n",
       " 'one.': 766,\n",
       " 'votes,': 442,\n",
       " 'win.': 225,\n",
       " 'sabotaged,': 2,\n",
       " 'ones.': 126,\n",
       " 'security.': 16,\n",
       " 'videocameras': 1,\n",
       " 'swaped.': 1,\n",
       " 'eliminated.': 12,\n",
       " 'efficient.': 85,\n",
       " 'posibilty': 3,\n",
       " 'reconizing': 6,\n",
       " 'adults.': 31,\n",
       " 'satifying': 1,\n",
       " 'better.': 779,\n",
       " 'reliable;': 2,\n",
       " 'bored\".': 59,\n",
       " '\"confused': 8,\n",
       " 'bored\"': 79,\n",
       " 'anything,': 113,\n",
       " 'learn;': 4,\n",
       " 'classroom.': 983,\n",
       " 'students.': 813,\n",
       " 'her/she': 1,\n",
       " 'assingment': 9,\n",
       " \"the're\": 5,\n",
       " 'doing.': 234,\n",
       " 'progam?': 3,\n",
       " 'II.': 127,\n",
       " 'them.': 3051,\n",
       " 'Administration,': 10,\n",
       " 'UNRRA.': 83,\n",
       " 'progam': 44,\n",
       " 'people,': 1112,\n",
       " 'world,': 626,\n",
       " 'fun.': 543,\n",
       " 'progam,': 3,\n",
       " 'work.': 961,\n",
       " 'yourself.': 199,\n",
       " 'says,': 965,\n",
       " 'needs.\"': 82,\n",
       " 'countries.': 283,\n",
       " 'good?': 16,\n",
       " 'program,': 199,\n",
       " 'going.': 125,\n",
       " 'Italy': 104,\n",
       " 'states,': 1731,\n",
       " '\"Besides': 51,\n",
       " 'China.\"': 48,\n",
       " 'exciting.': 32,\n",
       " 'working?': 21,\n",
       " 'animals.': 620,\n",
       " 'games.': 197,\n",
       " 'working,': 64,\n",
       " 'So,': 645,\n",
       " 'fun,': 222,\n",
       " 'world.': 2187,\n",
       " 'program.': 959,\n",
       " 'Europe.': 264,\n",
       " 'did,': 63,\n",
       " 'Cowboy?': 41,\n",
       " 'storie': 6,\n",
       " 'challeng': 10,\n",
       " 'auhor': 4,\n",
       " 'venus': 4150,\n",
       " 'earth.': 1057,\n",
       " 'dangers.': 512,\n",
       " 'worthy.': 44,\n",
       " '\"venus': 30,\n",
       " 'indisputable,': 37,\n",
       " 'productive.': 60,\n",
       " 'revals': 1,\n",
       " 'missions.': 13,\n",
       " 'taked': 7,\n",
       " 'pursuit.': 158,\n",
       " 'scientitis': 1,\n",
       " 'venusian': 56,\n",
       " 'Ths': 4,\n",
       " 'thta': 44,\n",
       " 'halping': 1,\n",
       " 'wiht': 26,\n",
       " 'vehicule.': 1,\n",
       " 'abouth': 4,\n",
       " 'danger.': 338,\n",
       " 'storis': 1,\n",
       " 'said\"': 8,\n",
       " 'presure': 31,\n",
       " 'geogoly': 1,\n",
       " 'presentadditional': 1,\n",
       " 'impedimants': 1,\n",
       " 'lihgtning': 1,\n",
       " 'surface.': 559,\n",
       " 'comparing,': 2,\n",
       " 'mechenical': 1,\n",
       " 'pressure,heat,and': 7,\n",
       " 'foces\".': 1,\n",
       " 'dager': 2,\n",
       " 'plante': 24,\n",
       " 'earthquakes.': 27,\n",
       " 'country.': 599,\n",
       " 'seas.': 98,\n",
       " 'Luke': 3069,\n",
       " '1945': 103,\n",
       " 'recover.': 18,\n",
       " '18': 270,\n",
       " 'service.': 134,\n",
       " 'UNRRA': 406,\n",
       " 'overseas.': 194,\n",
       " 'did.': 226,\n",
       " 'Atlantic': 299,\n",
       " 'coast.': 2,\n",
       " 'China,': 368,\n",
       " 'Europe,': 367,\n",
       " 'Greece,': 243,\n",
       " 'places.': 567,\n",
       " 'lot.': 166,\n",
       " 'seasick.': 5,\n",
       " 'adventures.': 62,\n",
       " 'family.': 123,\n",
       " 'in.': 691,\n",
       " 'traveling.': 46,\n",
       " 'idea,': 402,\n",
       " 'action.': 68,\n",
       " 'light?': 4,\n",
       " 'road.': 1054,\n",
       " 'humans?': 14,\n",
       " 'happens.': 133,\n",
       " 'malfunction.': 81,\n",
       " 'passenger.': 14,\n",
       " 'safe.': 833,\n",
       " 'moment.': 116,\n",
       " '\"It\\'s': 295,\n",
       " 'oppurtunity': 154,\n",
       " 'pollution,\"': 86,\n",
       " 'Car-free': 138,\n",
       " 'Bogota.': 74,\n",
       " 'cars,': 2703,\n",
       " 'pollution.': 668,\n",
       " 'German': 394,\n",
       " 'Suburb,': 281,\n",
       " '\"\"When': 5,\n",
       " 'tense.': 494,\n",
       " 'Im': 290,\n",
       " 'way,\"': 104,\n",
       " 'Heidrun': 382,\n",
       " 'Walter,': 176,\n",
       " 'two,': 179,\n",
       " 'motor.\"': 26,\n",
       " 'stress-': 2,\n",
       " '\"After': 107,\n",
       " 'near-record': 273,\n",
       " 'pollution,': 617,\n",
       " 'Paris': 1557,\n",
       " 'city;\"': 1,\n",
       " 'smog.': 582,\n",
       " 'used,': 101,\n",
       " 'air.': 546,\n",
       " 'Walking,': 16,\n",
       " 'biking,': 105,\n",
       " 'so.': 306,\n",
       " 'idea.': 1188,\n",
       " 'public.': 209,\n",
       " 'lives.': 517,\n",
       " 'neccessaties.': 1,\n",
       " 'assisstants': 2,\n",
       " 'drivers.': 412,\n",
       " 'alll': 22,\n",
       " 'First,': 543,\n",
       " 'BMW': 330,\n",
       " '\"Trafiic': 1,\n",
       " 'Assisstant\"': 2,\n",
       " '25': 826,\n",
       " 'all.': 1173,\n",
       " 'Driverless': 3898,\n",
       " 'drving': 84,\n",
       " 'smarter.': 32,\n",
       " 'fascinating.': 17,\n",
       " 'justy': 2,\n",
       " 'medical,': 2,\n",
       " 'possibilites': 23,\n",
       " 'endless.': 36,\n",
       " 'assisstant': 3,\n",
       " 'need,when': 1,\n",
       " 'wrong.': 592,\n",
       " 'sugnificant': 1,\n",
       " 'else.': 333,\n",
       " 'example,': 1765,\n",
       " '\"vibrates\"': 1,\n",
       " 'object.': 134,\n",
       " 'much.': 284,\n",
       " 'killed.': 39,\n",
       " 'Instance,': 4,\n",
       " 'parapraph': 2,\n",
       " '\"spinning': 3,\n",
       " '3-d': 22,\n",
       " 'surrounding.': 14,\n",
       " 'seat.': 50,\n",
       " 'more,': 266,\n",
       " '\"newer': 1,\n",
       " 'cars\"': 155,\n",
       " 'abilities,': 10,\n",
       " 'knowledge.': 69,\n",
       " \"doesn't.\": 18,\n",
       " 'Also,': 1378,\n",
       " 'lost.': 84,\n",
       " 'inculde,': 1,\n",
       " 'need.': 473,\n",
       " 'assisstance,': 1,\n",
       " 'located.': 16,\n",
       " 'unit.': 26,\n",
       " 'instance,': 335,\n",
       " 'station,': 8,\n",
       " 'hotel,': 1,\n",
       " 'restaurant,': 3,\n",
       " 'whereever': 4,\n",
       " 'be.': 754,\n",
       " 'navigate,': 5,\n",
       " 'save,': 6,\n",
       " 'easier.': 275,\n",
       " 'tremendoulsy': 1,\n",
       " 'rate.': 53,\n",
       " 'Last,': 79,\n",
       " 'already,': 35,\n",
       " 'now.': 899,\n",
       " 'important.': 171,\n",
       " 'colledge': 43,\n",
       " 'deserve.': 17,\n",
       " '538': 503,\n",
       " 'elecors': 4,\n",
       " '270': 358,\n",
       " '(who': 5,\n",
       " 'officials)': 1,\n",
       " 'paycheck,': 2,\n",
       " 'too,': 136,\n",
       " '60': 618,\n",
       " 'best.': 152,\n",
       " 'wnt': 2,\n",
       " 'is?': 60,\n",
       " '(or': 40,\n",
       " 'money).': 2,\n",
       " 'us.': 1180,\n",
       " 'something.': 322,\n",
       " 'electors,': 759,\n",
       " 'HEY!,': 1,\n",
       " 'do.': 1199,\n",
       " 'freedom,': 24,\n",
       " 'no,': 83,\n",
       " 'politicians.': 6,\n",
       " 'President!': 2,\n",
       " 'exsist': 21,\n",
       " 'death.': 127,\n",
       " 'accident,': 262,\n",
       " 'working.Or': 1,\n",
       " 'be?': 133,\n",
       " 'know.': 369,\n",
       " '9,': 106,\n",
       " '\"If': 291,\n",
       " 'injured,': 321,\n",
       " 'manufacture?\"': 9,\n",
       " 'injured.': 76,\n",
       " 'driverless.': 470,\n",
       " '\"None': 8,\n",
       " 'driverless.\"': 34,\n",
       " 'skills.\"': 25,\n",
       " 'sign,': 14,\n",
       " 'what?': 44,\n",
       " '\"Presently': 2,\n",
       " 'times.\"': 66,\n",
       " 'has100%': 1,\n",
       " 'driveless': 1578,\n",
       " 'accidents.': 848,\n",
       " 'Driveless': 396,\n",
       " 'sytem': 79,\n",
       " 'somthing.': 14,\n",
       " 'police,': 4,\n",
       " 'buses,': 88,\n",
       " 'pedestrians.': 60,\n",
       " 'drivless': 81,\n",
       " 'drive.': 720,\n",
       " 'farmers.': 4,\n",
       " 'aot': 3,\n",
       " 'comeup': 1,\n",
       " 'asking.': 8,\n",
       " 'sad,': 411,\n",
       " 'happy,': 859,\n",
       " 'truely': 151,\n",
       " 'feeling.': 887,\n",
       " 'then?': 64,\n",
       " 'techonlogy': 30,\n",
       " 'exctly': 1,\n",
       " 'someone,': 61,\n",
       " 'emotions,': 743,\n",
       " 'reasons.': 467,\n",
       " 'secert.': 1,\n",
       " '3-D': 720,\n",
       " 'face;': 140,\n",
       " 'technolgy': 297,\n",
       " 'delevopted': 1,\n",
       " 'Dr.': 1783,\n",
       " 'Huang': 1311,\n",
       " 'are.': 458,\n",
       " 'obseres': 1,\n",
       " '\"facial': 14,\n",
       " 'universal\"': 29,\n",
       " 'htought': 1,\n",
       " 'though?': 22,\n",
       " 'Dr': 80,\n",
       " 'Hoang': 1,\n",
       " '\"even': 61,\n",
       " 'verying': 1,\n",
       " 'expression.\"': 27,\n",
       " 'Mona': 1686,\n",
       " 'Lisa': 1234,\n",
       " 'disgusted?': 3,\n",
       " 'why?': 58,\n",
       " 'Leonardo': 138,\n",
       " 'de': 13,\n",
       " \"Vinci's\": 96,\n",
       " 'up?': 46,\n",
       " 'shower?': 1,\n",
       " 'benfits': 27,\n",
       " 'why.': 221,\n",
       " 'cool.': 123,\n",
       " 'movment': 10,\n",
       " 'idenitfy': 4,\n",
       " 'emotions.': 1787,\n",
       " 'Europe?': 5,\n",
       " 'Greece?': 2,\n",
       " 'Venice,': 281,\n",
       " 'Italy,': 198,\n",
       " 'Canal?': 1,\n",
       " 'things,': 358,\n",
       " 'material.': 72,\n",
       " 'teaches,': 2,\n",
       " 'experience,': 62,\n",
       " 'countries,': 183,\n",
       " 'own!': 4,\n",
       " 'of.': 271,\n",
       " 'something,': 170,\n",
       " 'disagree.': 48,\n",
       " 'normal,': 20,\n",
       " 'Yes,': 473,\n",
       " 'work,': 540,\n",
       " 'done.': 226,\n",
       " 'need,': 124,\n",
       " 'knowledge,': 34,\n",
       " 'Secondly,': 392,\n",
       " 'cover.': 5,\n",
       " 'over,': 149,\n",
       " 'reality,': 137,\n",
       " 'contrary.': 2,\n",
       " 'Cowboy,': 121,\n",
       " 'mentioned.': 15,\n",
       " 'China.': 585,\n",
       " 'unforgetable': 3,\n",
       " 'see!': 5,\n",
       " 'experience!': 6,\n",
       " 'forever!': 4,\n",
       " 'Plus,': 107,\n",
       " 'grandchildren,': 3,\n",
       " 'right?': 229,\n",
       " 'Finally,': 556,\n",
       " 'providing.': 3,\n",
       " 'military.': 32,\n",
       " 'so,': 248,\n",
       " 'lives,': 143,\n",
       " 'them?': 157,\n",
       " 'mess.': 28,\n",
       " 'III?': 1,\n",
       " 'Here,': 12,\n",
       " 'country,': 209,\n",
       " 'Cowboys,': 73,\n",
       " 'place.': 671,\n",
       " 'hard,': 57,\n",
       " 'chance,': 29,\n",
       " 'Panama!': 1,\n",
       " 'fro,': 2,\n",
       " 'mankind.': 30,\n",
       " 'for?': 106,\n",
       " 'today!': 39,\n",
       " 'garages,': 28,\n",
       " 'gastations,': 1,\n",
       " 'parkinglots.': 1,\n",
       " 'revovle': 1,\n",
       " 'many.': 60,\n",
       " 'cities,': 180,\n",
       " 'enviorment': 167,\n",
       " 'communities.': 96,\n",
       " 'Vauban,': 637,\n",
       " 'Germany': 569,\n",
       " 'compltely': 1,\n",
       " '(Rosenthal).': 36,\n",
       " 'otherhand,': 6,\n",
       " 'Paris,': 481,\n",
       " 'France': 450,\n",
       " 'smog-only': 1,\n",
       " '(Duffer).': 19,\n",
       " 'as,': 197,\n",
       " 'poplution': 6,\n",
       " 'tailpipes,': 10,\n",
       " 'lessened,': 2,\n",
       " 'indroduced': 1,\n",
       " 'transportaion': 52,\n",
       " 'differnet': 33,\n",
       " '\"Passenger': 118,\n",
       " 'car-intensive': 244,\n",
       " 'States\"': 33,\n",
       " 'usauge': 3,\n",
       " 'polution': 328,\n",
       " '\"Transportation': 5,\n",
       " \"America's\": 232,\n",
       " 'emissions\"': 19,\n",
       " 'U.S.': 387,\n",
       " 'ruduce': 11,\n",
       " 'warmimg': 1,\n",
       " 'towns.': 11,\n",
       " '\"Congestion': 102,\n",
       " 'France,': 316,\n",
       " 'smog\"': 114,\n",
       " 'well,': 287,\n",
       " 'Lastly,': 598,\n",
       " 'Bogota,': 657,\n",
       " 'Colombia': 422,\n",
       " 'bike,': 179,\n",
       " 'skateboard,': 7,\n",
       " '(Selsky).': 10,\n",
       " 'health.': 117,\n",
       " 'clear,': 38,\n",
       " 'America.': 307,\n",
       " 'movement.': 67,\n",
       " 'seatbelt': 10,\n",
       " 'today.': 692,\n",
       " 'should.': 82,\n",
       " 'robots,': 18,\n",
       " 'children.': 76,\n",
       " 'far.': 88,\n",
       " 'Indiana,': 5,\n",
       " 'Indianapolis': 2,\n",
       " 'resturant': 5,\n",
       " 'Indiana': 10,\n",
       " 'California,': 222,\n",
       " 'thing.': 706,\n",
       " 'malfuncting': 1,\n",
       " 'buliding': 11,\n",
       " 'general,': 31,\n",
       " 'itself,': 474,\n",
       " 'ot': 121,\n",
       " 'roads.': 384,\n",
       " 'bulid': 20,\n",
       " 'illlegal': 1,\n",
       " 'couple.': 1,\n",
       " 'expericence': 6,\n",
       " 'phone,': 66,\n",
       " 'anything.': 316,\n",
       " 'over.': 461,\n",
       " 'prevented.': 21,\n",
       " 'driving.': 1221,\n",
       " 'Mars,': 774,\n",
       " 'speeds.': 116,\n",
       " 'like.': 348,\n",
       " '97%': 229,\n",
       " 'dioixde': 1,\n",
       " 'blankets.': 38,\n",
       " \"Venu's\": 55,\n",
       " 'oceans.': 117,\n",
       " 'live,': 51,\n",
       " 'heat.': 91,\n",
       " '83': 277,\n",
       " 'disgusted,': 313,\n",
       " 'fearful,': 290,\n",
       " 'angry.': 277,\n",
       " '\"Dr.': 36,\n",
       " 'communicate.\"': 16,\n",
       " 'gald': 2,\n",
       " 'workd': 2,\n",
       " \"should'nt\": 52,\n",
       " 'pefrom': 2,\n",
       " 'life,': 901,\n",
       " 'proclaim.': 1,\n",
       " 'beter': 25,\n",
       " 'smily': 3,\n",
       " 'themself': 63,\n",
       " 'violence,': 3,\n",
       " 'argument,': 63,\n",
       " 'place,': 204,\n",
       " 'true,': 173,\n",
       " 'xepress': 1,\n",
       " 'out.': 732,\n",
       " \"D'Alto,\": 70,\n",
       " 'Nick,': 3,\n",
       " '\"Making': 428,\n",
       " 'Smile,\"': 113,\n",
       " 'Carus': 3,\n",
       " 'Company,': 31,\n",
       " 'November': 15,\n",
       " '2006.': 8,\n",
       " 'alot.': 80,\n",
       " 'wreacks': 3,\n",
       " 'right.': 282,\n",
       " 'safer.': 263,\n",
       " 'stae': 15,\n",
       " 'mind.': 157,\n",
       " 'im': 659,\n",
       " 'somthing': 191,\n",
       " 'years.': 715,\n",
       " 'learned.': 14,\n",
       " 'somone': 92,\n",
       " 'responcibale.': 1,\n",
       " 'devestated': 4,\n",
       " 'll.': 82,\n",
       " 'I,': 72,\n",
       " 'Bomberger,': 40,\n",
       " 'Cowboy.': 463,\n",
       " 'expirience': 17,\n",
       " 'hisoric': 1,\n",
       " '(the': 131,\n",
       " 'Administration)': 32,\n",
       " 'left.': 31,\n",
       " 'animals?': 23,\n",
       " 'contibute.': 1,\n",
       " 'appriciative.': 1,\n",
       " 'joining.': 12,\n",
       " 'year,': 119,\n",
       " 'friend,': 62,\n",
       " 'Reist,': 14,\n",
       " 'landmarks.': 13,\n",
       " 'special.': 71,\n",
       " 'Greece.': 180,\n",
       " 'water,': 129,\n",
       " 'treat.': 3,\n",
       " 'Crete': 193,\n",
       " 'on.': 771,\n",
       " 'Canal!': 3,\n",
       " 'Cowboys.': 337,\n",
       " 'tell,': 26,\n",
       " 'expirience.': 4,\n",
       " 'soon,': 51,\n",
       " 'system?': 42,\n",
       " 'expressions.': 487,\n",
       " 'indivuals': 2,\n",
       " 'mordern': 8,\n",
       " 'expressional': 1,\n",
       " 'peers,friends,and': 1,\n",
       " 'term.': 16,\n",
       " 'syndromes.': 2,\n",
       " 'Smile\"': 239,\n",
       " \"D'Alto\": 176,\n",
       " '\"Of': 23,\n",
       " 'course,': 191,\n",
       " 'happy,worried,': 3,\n",
       " 'etc.\"': 30,\n",
       " 'diffucult.': 2,\n",
       " 'day.': 1341,\n",
       " 'student.': 379,\n",
       " 'bored,': 384,\n",
       " 'predicts.': 205,\n",
       " '\"Then': 183,\n",
       " 'lesson,': 740,\n",
       " 'insturctor.\"': 1,\n",
       " 'online.': 36,\n",
       " 'technology.': 1027,\n",
       " 'benefical': 53,\n",
       " 'assignment.': 35,\n",
       " 'help.': 593,\n",
       " \"''The\": 45,\n",
       " \"Venus''\": 9,\n",
       " 'text,': 249,\n",
       " \"''Evening\": 12,\n",
       " \"Star''is\": 1,\n",
       " 'sky,In': 1,\n",
       " 'systen': 6,\n",
       " 'secont': 2,\n",
       " 'sun;': 2,\n",
       " 'teh': 64,\n",
       " 'Earth,': 910,\n",
       " 'Mars.': 1858,\n",
       " 'planet,': 758,\n",
       " 'somre': 1,\n",
       " 'persent': 29,\n",
       " 'dioxide,Venus': 1,\n",
       " 'sumfor': 1,\n",
       " 'iur': 2,\n",
       " 'temoperature': 1,\n",
       " 'fahrenheit,': 91,\n",
       " '90': 1224,\n",
       " 'survive.': 172,\n",
       " 'scientifict': 1,\n",
       " 'povide': 1,\n",
       " 'th': 204,\n",
       " 'eatmosphere': 1,\n",
       " 'dvideography': 1,\n",
       " 'ineffective.': 72,\n",
       " 'proyects': 2,\n",
       " 'doo': 3,\n",
       " \"1800's\": 37,\n",
       " \"1940's\": 27,\n",
       " 'll,': 8,\n",
       " 'fon': 1,\n",
       " 'computer.': 346,\n",
       " 'conclution,': 27,\n",
       " 'fron': 7,\n",
       " 'interestings': 3,\n",
       " 'scientificts,': 1,\n",
       " 'space,,': 1,\n",
       " 'scientuifics': 1,\n",
       " 'lookk': 1,\n",
       " 'interesting.': 112,\n",
       " 'todays': 271,\n",
       " 'car,': 1772,\n",
       " 'usage.': 691,\n",
       " 'environment.': 597,\n",
       " 'transportation.': 570,\n",
       " \"'Car-Free\": 3,\n",
       " \"Cities'\": 2,\n",
       " 'advantanges': 3,\n",
       " '\"Heidrun': 3,\n",
       " 'occaional': 1,\n",
       " 'said,': 545,\n",
       " 'active.': 46,\n",
       " 'Furthermore,': 289,\n",
       " 'respondible': 3,\n",
       " 'Europe...': 70,\n",
       " 'States.\"': 82,\n",
       " 'usages,': 6,\n",
       " '\"Vauban,': 18,\n",
       " 'mile,': 33,\n",
       " 'low-car': 49,\n",
       " 'life.\"': 34,\n",
       " 'Moreover,': 63,\n",
       " '\"But': 95,\n",
       " 'transportation,': 312,\n",
       " 'parking.\"': 3,\n",
       " '\"In': 529,\n",
       " 'approach,': 48,\n",
       " 'away,': 175,\n",
       " 'street,': 124,\n",
       " 'highway.\"': 22,\n",
       " 'destination.': 207,\n",
       " 'money,': 360,\n",
       " 'gas.': 297,\n",
       " 'walks,': 14,\n",
       " 'exercise.': 74,\n",
       " 'trails,': 3,\n",
       " 'parks,': 32,\n",
       " 'hit.': 33,\n",
       " '\"The': 2681,\n",
       " 'Bogota': 512,\n",
       " 'mid-1990s.\"': 4,\n",
       " '\"It': 348,\n",
       " '118': 146,\n",
       " 'bicylce': 3,\n",
       " 'paths.\"': 3,\n",
       " 'humans,': 241,\n",
       " 'breathe.': 53,\n",
       " 'active,': 18,\n",
       " '\"Parks': 64,\n",
       " 'city;': 84,\n",
       " 'uneven,': 80,\n",
       " 'broad,': 107,\n",
       " 'sidewalks;': 78,\n",
       " 'rush-hour': 113,\n",
       " 'traffic;\"': 1,\n",
       " 'bike.': 92,\n",
       " 'money.': 743,\n",
       " 'ways.': 566,\n",
       " 'distance,': 61,\n",
       " 'decision.': 94,\n",
       " 'challege': 17,\n",
       " 'size.': 282,\n",
       " 'poin': 10,\n",
       " 'earth,': 267,\n",
       " 'closely.': 145,\n",
       " 'explore.': 171,\n",
       " 'levels.\"Venus': 1,\n",
       " 'system,': 905,\n",
       " 'sun\".': 17,\n",
       " 'Fahrenheit,but': 7,\n",
       " 'Earth.\"However,': 1,\n",
       " 'atmosphere,rendering': 9,\n",
       " 'atandard': 1,\n",
       " 'videography': 127,\n",
       " 'stressed,': 28,\n",
       " 'valued.': 6,\n",
       " 'showing.': 23,\n",
       " 'education.': 140,\n",
       " 'stressed.': 42,\n",
       " 'FACS': 1956,\n",
       " 'break.': 50,\n",
       " 'classroom,': 289,\n",
       " 'amongst': 42,\n",
       " 'peers.': 24,\n",
       " 'students,': 323,\n",
       " 'acknowleding': 1,\n",
       " 'attendence,': 1,\n",
       " 'valued,': 2,\n",
       " 'feelings.': 176,\n",
       " 'upset,': 66,\n",
       " 'ninety-seven': 16,\n",
       " 'venus.': 763,\n",
       " 'high,': 55,\n",
       " 'well.': 1192,\n",
       " 'eight-hundred': 14,\n",
       " 'fahrenheit': 150,\n",
       " 'Althrough,': 1,\n",
       " 'extreme.': 24,\n",
       " 'stated,': 154,\n",
       " 'Earth-like': 670,\n",
       " 'system.\"': 179,\n",
       " 'features,': 96,\n",
       " 'fetures': 18,\n",
       " 'valley,': 11,\n",
       " 'mountains,': 606,\n",
       " 'craters.\"': 128,\n",
       " 'Nasa': 315,\n",
       " 'simplifield': 2,\n",
       " \"Venus's\": 1134,\n",
       " 'conditions.\"': 114,\n",
       " 'research.': 127,\n",
       " 'Overall,': 273,\n",
       " 'places?': 26,\n",
       " 'eye-opening': 3,\n",
       " 'essay,': 66,\n",
       " 'Two,': 31,\n",
       " 'repayment.': 1,\n",
       " 'food.': 86,\n",
       " 'trips(the': 2,\n",
       " 'Cowboy),': 1,\n",
       " 'youself': 19,\n",
       " 'spirits.': 1,\n",
       " 'overseas,': 9,\n",
       " 'sites.': 44,\n",
       " 'opprotunities': 7,\n",
       " 'water.': 218,\n",
       " 'scrificed': 1,\n",
       " 'home,': 142,\n",
       " 'as:fencing,': 1,\n",
       " 'boxing,': 348,\n",
       " 'reading,': 308,\n",
       " 'whittling,': 186,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302</th>\n",
       "      <td>ffd378d</td>\n",
       "      <td>the story \" The Challenge of Exploing Venus \" ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>ffddf1f</td>\n",
       "      <td>Technology has changed a lot of ways that we l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17304</th>\n",
       "      <td>fff016d</td>\n",
       "      <td>If you don't like sitting around all day than ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17305</th>\n",
       "      <td>fffb49b</td>\n",
       "      <td>In \"The Challenge of Exporing Venus,\" the auth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17306</th>\n",
       "      <td>fffed3e</td>\n",
       "      <td>Venus is worthy place to study but dangerous. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17307 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                          full_text  score\n",
       "0      000d118  Many people have car where they live. The thin...      3\n",
       "1      000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2      001ab80  People always wish they had the same technolog...      4\n",
       "3      001bdc0  We all heard about Venus, the planet without a...      4\n",
       "4      002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3\n",
       "...        ...                                                ...    ...\n",
       "17302  ffd378d  the story \" The Challenge of Exploing Venus \" ...      2\n",
       "17303  ffddf1f  Technology has changed a lot of ways that we l...      4\n",
       "17304  fff016d  If you don't like sitting around all day than ...      2\n",
       "17305  fffb49b  In \"The Challenge of Exporing Venus,\" the auth...      1\n",
       "17306  fffed3e  Venus is worthy place to study but dangerous. ...      2\n",
       "\n",
       "[17307 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\",enable=['parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./learning-agency-lab-automated-essay-scoring-2/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47 s ± 527 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "docs= list()\n",
    "for doc in nlp.pipe(data['full_text'].sample(200),batch_size=100,n_process=8):\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.78 s ± 159 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "docs= list()\n",
    "for doc in nlp.pipe(data['full_text'].sample(500),batch_size=20,n_process=8):\n",
    "    p_doc = nlp(doc)\n",
    "    docs.append([sent.text for sent in p_doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocs= list()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfor doc in data[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m].sample(200):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    p_doc = nlp(doc)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    docs.append([sent.text for sent in p_doc.sents])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/essay_scoring/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/github/essay_scoring/.conda/lib/python3.11/site-packages/IPython/core/magics/execution.py:1189\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1187\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1191\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m~/github/essay_scoring/.conda/lib/python3.11/timeit.py:208\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    206\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 208\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/github/essay_scoring/.conda/lib/python3.11/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:3\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/github/essay_scoring/.conda/lib/python3.11/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "docs= list()\n",
    "for doc in data['full_text'].sample(200):\n",
    "    p_doc = nlp(doc)\n",
    "    docs.append([sent.text for sent in p_doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sentences = []\n",
    "        # Adding batching as higher number of text causing memory issues & slowing down multiprocessing throughput\n",
    "        for doc in tqdm(nlp.pipe(self.text,batch_size=100,n_process=8),total=len(self.text)):\n",
    "            sentences.append([sent.text for sent in doc.sents])\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
